#define memoryAlloc(s uint) ptr
#define memoryRealloc(o ptr, s uint) ptr
#define memoryAllocSTR(s uint) str

#include "thread.bah"

const SIGGCSTOP = 34

#define pthread_kill(t pthread_t, sig int32) int32

#debug {
    #define getTimeUnix() int
}

GC_min_collect = 128000000
VERBOSE_GC = false

GC_min_ptr = 16777216
GC_max_ptr = 281474976710656

#define qsort(arr ptr, elementCount uint, elementSize uint, fn ptr)
#define itoa(i int) str
#define syscall(nb int) int

#define printInt(i int)
#define backtrace(l uint)


struct BAH_GC_ptr {
    size: uint
    p: uint
    marked: bool
    notScannable: bool
}

struct BAH_GC_thread {
    t: pthread_t
    sysid: uint
    stackBase: uint
    regs: buffer:184


    next: BAH_GC_thread*

    clean() {
        free(this)
    }

}

struct BAH_GC_page {
    page: uint
    list: BAH_GC_ptr*
    listLength: uint
    listRealLength: uint

    next: BAH_GC_page*

    addPtr(p uint) BAH_GC_ptr* {
        if this.listLength == this.listRealLength {
            this.listRealLength *= 2
            this.list = realloc(this.list, sizeof(BAH_GC_ptr) * this.listRealLength)
            sizeDiff = (this.listRealLength / 2) * sizeof(BAH_GC_ptr)
            memset(<ptr>(<uint>this.list + sizeDiff), 0, sizeDiff)
        }

        index BAH_GC_ptr*
        this.listLength++

        i = this.listLength - 1
        for i != -1, i-- {
            index = <BAH_GC_ptr*>(<uint>this.list + i * sizeof(BAH_GC_ptr))
            if index.p == 0 {
                break
            }
        }

        if i == -1 {
            panic("GC: fatal error")
        }

        index.p = p
        return index
    }

    find(p uint) BAH_GC_ptr* {
        low = 0
        high = this.listLength - 1

        for low <= high {
            mid = (low + high) / 2
            index = <BAH_GC_ptr*>(<uint>this.list + mid * sizeof(BAH_GC_ptr))

            if index.p == 0 {
                panic("GC: corrupted ptr list.")
            }

            if index.p == p {
                return index
            }

            if index.p < p {
                high = mid - 1
            } else {
                low = mid + 1
            }

        }

        return null
    }
}


struct BAH_GC_channelNode {
    data: BAH_GC_ptr*
    next: BAH_GC_channelNode*
}

struct BAH_GC_state_tag {
    heapSize: uint
    nextCollect: uint = GC_min_collect

    pages: BAH_GC_page*

    stackBase: uint
    threads: BAH_GC_thread*
    
    mainThread: int

    mut: pthread_mutex_t*

    atomicThreadCount: uint
    atomicResume:      int

    scanStackBase: uint
    scanRBP: uint

    overflowQueue: BAH_GC_channelNode*

    isPointerInBounds(p uint) BAH_GC_page * {
        if p < GC_min_ptr {
            return null
        }
        if p > GC_max_ptr {
            return null
        }
        pageNb = <uint32>(p >> 21)

        currPage = this.pages
        for currPage != null, currPage = currPage.next {
            if pageNb == currPage.page {
                return currPage
            }
        }

        return null
    }

    updatePagesBounds(p uint) BAH_GC_page* {
        pageNb = <uint32>(p >> 21)

        currPage = this.pages
        for currPage != null, currPage = currPage.next {
            if pageNb == currPage.page {
                break
            }
        }

        if currPage == null {
            currPage = malloc(sizeof(BAH_GC_page))
            currPage.page = pageNb
            currPage.next = this.pages
            this.pages = currPage

            currPage.listLength = 0
            currPage.listRealLength = 4096
            s = currPage.listRealLength * sizeof(BAH_GC_ptr)
            currPage.list = malloc(s)
            memset(currPage.list, 0, s)

            if p < GC_min_ptr {
                GC_min_ptr = p
            }

            if p > GC_max_ptr {
                GC_max_ptr = p
            }
        }

        return currPage
    }

    allocate(s uint, notScannable bool) uint {
        p = <uint>malloc(s)

        if p == 0 {
            panic("GC: could not allocate memory.")
        }


        this.heapSize += s

        ptrPage = this.updatePagesBounds(p)

        index = ptrPage.addPtr(p)

        index.size = s
        index.notScannable = notScannable

        return p
    }
}


GC_sort_pointers(a BAH_GC_ptr*, b BAH_GC_ptr*) int {
    if a.p < b.p {
        return 1
    }

    return -1
}

BAH_GC_state BAH_GC_state_tag

GC_setHeapSize(size uint) {
    GC_min_collect = size
    BAH_GC_state.nextCollect = size
}

#define GC_scan(from uint, to uint, alignment uint)
#define real_pthread_exit(val ptr)

GC_thread_wrapper(args ptr*) {

    t = <BAH_GC_thread*>*args

    pthread_mutex_lock(BAH_GC_state.mut)
    t.stackBase = <uint>$rbp
    t.next = BAH_GC_state.threads
    t.t = pthread_self()
    t.sysid = syscall(__NR_gettid)
    BAH_GC_state.threads = t
    pthread_mutex_unlock(BAH_GC_state.mut)

    fn function(ptr) = *(<ptr*>(<uint>args + 16))

    fnArg = *(<ptr*>(<uint>args + sizeof(ptr)))
    free(args)

    fn(fnArg)

    //remove the thread from the linked list
    pthread_mutex_lock(BAH_GC_state.mut)
    if t == BAH_GC_state.threads {
        BAH_GC_state.threads = t.next
        t.clean()
    } else {
        ct = BAH_GC_state.threads; for ct != null, ct = ct.next {
            if ct.next == t {
                ct.next = t.next
                t.clean()
                break
            }
        }
    }
    pthread_mutex_unlock(BAH_GC_state.mut)
}

GC_removeCurrentThread(val ptr) {
    //remove the thread from the linked list
    callerId = syscall(__NR_gettid)

    pthread_mutex_lock(BAH_GC_state.mut)
    if BAH_GC_state.threads.sysid == callerId {
        t = BAH_GC_state.threads
        BAH_GC_state.threads = BAH_GC_state.threads.next
        t.clean()
    } else {
        currT = BAH_GC_state.threads
        for currT != null, currT = currT.next {
            if currT.next.sysid == callerId {
                t = currT.next
                currT.next = t.next
                t.clean()
                break
            }
        }
    }

    pthread_mutex_unlock(BAH_GC_state.mut)
    real_pthread_exit(val)
}

#define GC_scanReversed(p uint, to uint, alignment uint)
#define GC_scan(p uint, to uint, alignment uint)


GC_thread_sig_handler(sig int, info ptr, ctx ptr) {
    callerId = syscall(__NR_gettid)

    t = BAH_GC_state.threads
    for t != null, t = t.next {
        if t.sysid == callerId {
            break
        }
    }

    memcpy(<ptr>t.regs, <ptr>(<uint>ctx + 40), 184)

    atomic_add(&BAH_GC_state.atomicThreadCount, 1)

    for atomic_load(&BAH_GC_state.atomicResume) == 0 {
        sched_yield()
    }
}

GC_thread_create(id pthread_t*, attr pthread_attr_t*, func ptr, args ptr) int32 {
    t = <BAH_GC_thread*>malloc(sizeof(BAH_GC_thread))
    if id == null {
        id = &t.t
    }

    agrsWrapper = <ptr*>malloc(sizeof(ptr) * 3)
    
    argsW = <ptr*>(<uint>agrsWrapper + sizeof(ptr))
    fnW = <ptr*>(<uint>agrsWrapper + 2 * sizeof(ptr))

    *agrsWrapper = t
    *argsW = args
    *fnW = func

    errNo = pthread_create(id, attr, GC_thread_wrapper, agrsWrapper)

    return errNo
}

GC_thread_create_detached(id pthread_t*, attr pthread_attr_t*, func ptr, args ptr) int32 {
    t = <BAH_GC_thread*>malloc(sizeof(BAH_GC_thread))
    if id == null {
        id = &t.t
    }

    agrsWrapper = <ptr*>malloc(sizeof(ptr) * 3)
    
    argsW = <ptr*>(<uint>agrsWrapper + sizeof(ptr))
    fnW = <ptr*>(<uint>agrsWrapper + 2 * sizeof(ptr))

    *agrsWrapper = t
    *argsW = args
    *fnW = func

    realAttr pthread_attr_t

    if attr != null {
        realAttr = *attr
    } else {
        pthread_attr_init(&realAttr)
    }

    pthread_attr_setdetachstate(&realAttr, 1)

    errNo = pthread_create(id, &realAttr, GC_thread_wrapper, agrsWrapper)

    pthread_attr_destroy(&realAttr)


    return errNo
}


GC_thread_join(id pthread_t, ret ptr) int32 {
    return pthread_join(id, ret)
}

GC_setStackBase() {
    BAH_GC_state.stackBase = <uint>$stackBase
}

#define GC_thread_worker()

GC_init() {
    BAH_GC_state.mainThread = syscall(__NR_gettid)
    BAH_GC_state.nextCollect = GC_min_collect

    action = sigaction {
        sa_handler: GC_thread_sig_handler
        sa_flags: SA_SIGINFO | SA_RESTART
    }
    sigaction(SIGGCSTOP, &action, null)

    BAH_GC_state.mut = malloc(sizeof(pthread_mutex_t))

    pthread_mutex_init(BAH_GC_state.mut, 0)

    BAH_GC_state.threads = malloc(sizeof(BAH_GC_thread))
    BAH_GC_state.threads.sysid = BAH_GC_state.mainThread
    BAH_GC_state.threads.stackBase = BAH_GC_state.stackBase
    BAH_GC_state.threads.t = pthread_self()
}

#init GC_setStackBase()
#init GC_init()


GC_markPtr(p uint) {
    if p == 0 {
        return
    }


    ptrPage = BAH_GC_state.isPointerInBounds(p)

    if ptrPage == null {
        return
    }

    gcPtr = ptrPage.find(p)

    if gcPtr == null {
        return
    }
    if gcPtr.marked {
        return
    }


    gcPtr.marked = true
    if gcPtr.notScannable == false {
        if BAH_GC_state.scanStackBase - BAH_GC_state.scanRBP >= 6000000 {
            overflowNode = <BAH_GC_channelNode*>malloc(sizeof(BAH_GC_channelNode))
            overflowNode.data = gcPtr
            overflowNode.next = BAH_GC_state.overflowQueue
            BAH_GC_state.overflowQueue = overflowNode
            return
        }
        GC_scan(p, p + gcPtr.size, 1)
    }
}


GC_scan(p uint, to uint, alignment uint) {
    for p < to - 7, p = p + alignment {
        GC_markPtr(*<uint*>p)
    }
}

GC_scanReversed(p uint, to uint, alignment uint) {
    for p >= to, p = p - alignment {
        GC_markPtr(*<uint*>p)
    }
}

GC_stopWorld(callerId uint) uint {
    t = BAH_GC_state.threads

    nb = 0
    for t != null, t = t.next {
        if t.sysid == callerId {
            continue
        }
        if pthread_kill(t.t, SIGGCSTOP) < 0 {
            continue
        }
        nb++
    }

    return nb
}

GC_collect() {
    //save registers
    ordi = <uint>$rdi
    orsi = <uint>$rsi
    ordx = <uint>$rdx
    orcx = <uint>$rcx
    or8 = <uint>$r8
    or9 = <uint>$r9
    or10 = <uint>$r10
    or11 = <uint>$r11
    or12 = <uint>$r12
    or13 = <uint>$r13
    or14 = <uint>$r14
    or15 = <uint>$r15
    orax = <uint>$rax
    orbx = <uint>$rbx

    currBasePointer = <uint>$rbp

    #debug {
        if VERBOSE_GC {
            puts("[GC] Collecting.")
        }
        scanStart = getTimeUnix()
    }

    currPage = BAH_GC_state.pages
    for currPage != null, currPage = currPage.next {
        qsort(currPage.list, currPage.listRealLength, sizeof(BAH_GC_ptr), GC_sort_pointers)
    }

    callerId = syscall(__NR_gettid)

    currThread = BAH_GC_state.threads
    for currThread != null, currThread = currThread.next {
        if currThread.sysid == callerId {
            break
        }
    }

    BAH_GC_state.scanStackBase = currThread.stackBase
    BAH_GC_state.scanRBP = currBasePointer

    atomic_store(&BAH_GC_state.atomicResume, 0)
    atomic_store(&BAH_GC_state.atomicThreadCount, 0)

    nbThreads = GC_stopWorld(callerId)

    //check registers
    GC_markPtr(ordi)
    GC_markPtr(orsi)
    GC_markPtr(ordx)
    GC_markPtr(orcx)
    GC_markPtr(or8)
    GC_markPtr(or9)
    GC_markPtr(or10)
    GC_markPtr(or11)
    GC_markPtr(or12)
    GC_markPtr(or13)
    GC_markPtr(or14)
    GC_markPtr(or15)
    GC_markPtr(orax)
    GC_markPtr(orbx)

    //check .data and .bss
    dataStart = <uint>$data
    bssStart = <uint>$bss
    bssEnd = <uint>$end

    GC_scan(dataStart, bssStart, 1)
    GC_scan(bssStart, bssEnd, 1)

    //check stack
    GC_scanReversed(currThread.stackBase, currBasePointer, 8)

    for atomic_load(&BAH_GC_state.atomicThreadCount) != nbThreads {
        sched_yield()
    }

    ct = BAH_GC_state.threads
    for ct != null, ct = ct.next {
        if ct.sysid == callerId {
            continue
        }
        GC_scanReversed(ct.stackBase, (*<uint*>(<uint>ct.regs + 120)), 8)
        GC_scan(<uint>ct.regs, <uint>ct.regs + 184, 8)
    }

    freed=0
    alive=0

    additional=0
    for BAH_GC_state.overflowQueue != null {
        currQueue = BAH_GC_state.overflowQueue
        BAH_GC_state.overflowQueue = null
        for currQueue != null {
            GC_scan(currQueue.data.p, currQueue.data.p + currQueue.data.size, 1)
            prevQueue = currQueue
            currQueue = currQueue.next
            free(prevQueue)
            additional++
        }
    }

    currPage = BAH_GC_state.pages
    lastPage = <BAH_GC_page*>null
    for currPage != null {
        i=currPage.listLength - 1; for i != -1 , i-- {
            index = <BAH_GC_ptr*>(<uint>currPage.list + i * sizeof(BAH_GC_ptr))
            if index.p == 0 {
                continue
            }

            if index.marked == false {
                freed++
                free(<ptr>index.p)
                lastIndex = <BAH_GC_ptr*>(<uint>currPage.list + (currPage.listLength - 1) * sizeof(BAH_GC_ptr))
                *index = *lastIndex
                lastIndex.p = 0
                BAH_GC_state.heapSize -= index.size
                currPage.listLength--
                continue
            }

            alive++

            index.marked = false
        }

        if currPage.listLength == 0 {
            if lastPage == null {
                BAH_GC_state.pages = currPage.next
                next = currPage.next
                free(currPage.list)
                free(currPage)
                currPage = next
            } else {
                lastPage.next = currPage.next
                free(currPage.list)
                free(currPage)
                currPage = lastPage.next
            }
            continue
        }

        qsort(currPage.list, currPage.listLength, sizeof(BAH_GC_ptr), GC_sort_pointers)
        lastPage = currPage
        currPage = currPage.next
    }

    #debug {
        if VERBOSE_GC {

            nbPages = 0
            currPage = BAH_GC_state.pages
            for currPage != null, currPage = currPage.next {
                nbPages++
            }

            puts("===COLLECT RESULTS===")
            puts("freed: ")
            printInt(freed)
            puts("still allive: ")
            printInt(alive)
            puts("heap size:")
            printInt(BAH_GC_state.heapSize)
            puts("nb pages:")
            printInt(nbPages)
            puts("took (ms):")
            printInt((getTimeUnix() - scanStart) / 1000000)
            puts("=====================")
        }
    }

    atomic_store(&BAH_GC_state.atomicResume, 1)
}

memoryAlloc(s uint) ptr {
    pthread_mutex_lock(BAH_GC_state.mut)
    if BAH_GC_state.heapSize >= BAH_GC_state.nextCollect {
        GC_collect()
        BAH_GC_state.nextCollect = BAH_GC_state.heapSize * 2
        if BAH_GC_state.nextCollect < GC_min_collect {
            BAH_GC_state.nextCollect = GC_min_collect
        }
    }


    r = BAH_GC_state.allocate(s, false)
    memset(<ptr>r, 0, s)

    pthread_mutex_unlock(BAH_GC_state.mut)
    return <ptr>r
}

memoryAlloc_NZ(s uint) ptr {
    pthread_mutex_lock(BAH_GC_state.mut)
    if BAH_GC_state.heapSize >= BAH_GC_state.nextCollect {
        GC_collect()
        BAH_GC_state.nextCollect = BAH_GC_state.heapSize * 2
        if BAH_GC_state.nextCollect < GC_min_collect {
            BAH_GC_state.nextCollect = GC_min_collect
        }
    }


    r = BAH_GC_state.allocate(s, false)

    *(<byte*>(r + s - 1)) = <byte>0

    pthread_mutex_unlock(BAH_GC_state.mut)
    return <ptr>r
}

memoryRealloc(o ptr, s uint) ptr {
    if o == null {
        return memoryAlloc(s)
    }

    pthread_mutex_lock(BAH_GC_state.mut)
    if BAH_GC_state.heapSize >= BAH_GC_state.nextCollect {
        GC_collect()
        BAH_GC_state.nextCollect = BAH_GC_state.heapSize * 2
        if BAH_GC_state.nextCollect < GC_min_collect {
            BAH_GC_state.nextCollect = GC_min_collect
        }
    }

    
    index BAH_GC_ptr*
    currPage = BAH_GC_state.isPointerInBounds(<uint>o)

    if currPage == null {
        panic("GC: cannot reallocate memory that was not allocated by the garbage collector.")
    }

    i=currPage.listRealLength-1; for i != -1, i-- {
        index = <BAH_GC_ptr*>(<uint>currPage.list + i * sizeof(BAH_GC_ptr))
        if index.p == <uint>o {
            break
        }
    }

    if i == -1 {
        panic("GC: fatal error doing reallocation")
    }

    r = <uint>realloc(o, s)

    if r == 0 {
        panic("GC: could not reallocate memory.")
    }

    memset(<ptr>(<uint>r + index.size), 0, s - index.size)

    BAH_GC_state.heapSize += s - index.size
    index.size = s

    if r != <uint>o {
        index.p = r
        newPage = BAH_GC_state.updatePagesBounds(r)
        if newPage != currPage {
            newIndex = newPage.addPtr(r)
            newIndex.size = index.size
            newIndex.notScannable = index.notScannable
            lastIndex = <BAH_GC_ptr*>(<uint>currPage.list + (currPage.listLength - 1) * sizeof(BAH_GC_ptr))
            *index = *lastIndex
            lastIndex.p = 0
            currPage.listLength--
        }
    }

    pthread_mutex_unlock(BAH_GC_state.mut)
    return <ptr>r
}


memoryAllocSTR(s uint) str {
    pthread_mutex_lock(BAH_GC_state.mut)
    if BAH_GC_state.heapSize >= BAH_GC_state.nextCollect {
        GC_collect()
        BAH_GC_state.nextCollect = BAH_GC_state.heapSize * 2
        if BAH_GC_state.nextCollect < GC_min_collect {
            BAH_GC_state.nextCollect = GC_min_collect
        }
    }


    r = BAH_GC_state.allocate(s, true)
    memset(<ptr>r, 0, s)
    pthread_mutex_unlock(BAH_GC_state.mut)
    return <ptr>r
}

memoryAllocSTR_NZ(s uint) str {
    pthread_mutex_lock(BAH_GC_state.mut)
    if BAH_GC_state.heapSize >= BAH_GC_state.nextCollect {
        GC_collect()
        BAH_GC_state.nextCollect = BAH_GC_state.heapSize * 2
        if BAH_GC_state.nextCollect < GC_min_collect {
            BAH_GC_state.nextCollect = GC_min_collect
        }
    }


    r = BAH_GC_state.allocate(s, true)
    *(<byte*>(r + s - 1)) = <byte>0
    pthread_mutex_unlock(BAH_GC_state.mut)
    return <ptr>r
}